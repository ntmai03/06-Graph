{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo:\n",
    "    \n",
    "    1. Review current notebook\n",
    "    2. Review the anlaysis of Credit card for understanding data and filling missing data\n",
    "    2. Add Logistic Regression using stats model\n",
    "    3. Add Feature Selection in Credit card analysis\n",
    "    4. Review and add methods of reference Feature Selection\n",
    "    5. Try with other datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A13</th>\n",
       "      <th>A14</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15776156</td>\n",
       "      <td>1</td>\n",
       "      <td>22.08</td>\n",
       "      <td>11.46</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.585</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1213</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15739548</td>\n",
       "      <td>0</td>\n",
       "      <td>22.67</td>\n",
       "      <td>7.00</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15662854</td>\n",
       "      <td>0</td>\n",
       "      <td>29.58</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>280</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15687688</td>\n",
       "      <td>0</td>\n",
       "      <td>21.67</td>\n",
       "      <td>11.50</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15715750</td>\n",
       "      <td>1</td>\n",
       "      <td>20.17</td>\n",
       "      <td>8.17</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1.960</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  A1     A2     A3  A4  A5  A6     A7  A8  A9  A10  A11  A12  \\\n",
       "0    15776156   1  22.08  11.46   2   4   4  1.585   0   0    0    1    2   \n",
       "1    15739548   0  22.67   7.00   2   8   4  0.165   0   0    0    0    2   \n",
       "2    15662854   0  29.58   1.75   1   4   4  1.250   0   0    0    1    2   \n",
       "3    15687688   0  21.67  11.50   1   5   3  0.000   1   1   11    1    2   \n",
       "4    15715750   1  20.17   8.17   2   6   4  1.960   1   1   14    0    2   \n",
       "\n",
       "   A13   A14  Class  \n",
       "0  100  1213      0  \n",
       "1  160     1      0  \n",
       "2  280     1      0  \n",
       "3    0     1      1  \n",
       "4   60   159      1  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('Credit_Card_Applications.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Separate train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((483, 14), (207, 14))"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(['CustomerID','Class'], axis=1),\n",
    "    df['Class'],\n",
    "    test_size = 0.3,\n",
    "    random_state = 0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11',\n",
       "       'A12', 'A13', 'A14'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I keep a copy of the dataset with all the variables\n",
    "# to measure the performance of machine learning models\n",
    "# at the end of the notebook\n",
    "\n",
    "X_train_original = X_train.copy()\n",
    "X_test_original = X_test.copy()\n",
    "\n",
    "original_features = X_train_original.columns\n",
    "original_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Filter method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Remove constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "\n",
      "['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12', 'A13', 'A14']\n"
     ]
    }
   ],
   "source": [
    "# remove constant features\n",
    "constant_features = [\n",
    "    feat for feat in X_train.columns if X_train[feat].std() == 0\n",
    "]\n",
    "features_to_keep = [var for var in X_train.columns if var not in constant_features]\n",
    "\n",
    "print(constant_features)\n",
    "print()\n",
    "print(features_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Remove quasi-constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove quasi-constant features\n",
    "sel = VarianceThreshold(\n",
    "    threshold=0.01)  # 0.1 indicates 99% of observations approximately\n",
    "\n",
    "sel.fit(X_train)  # fit finds the features with low variance\n",
    "\n",
    "sum(sel.get_support()) # how many not quasi-constant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11',\n",
       "       'A12', 'A13', 'A14'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_keep = X_train.columns[sel.get_support()]\n",
    "features_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn transformations lead to numpy arrays\n",
    "# here I transform the arrays back to dataframes\n",
    "# please be mindful of getting the columns assigned\n",
    "# correctly\n",
    "\n",
    "X_train= pd.DataFrame(X_train)\n",
    "X_train.columns = features_to_keep\n",
    "\n",
    "X_test= pd.DataFrame(X_test)\n",
    "X_test.columns = features_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Remove duplicated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicated features in the training set\n",
    "duplicated_feat = []\n",
    "for i in range(0, len(X_train.columns)):\n",
    "    if i % 10 == 0:  # this helps me understand how the loop is going\n",
    "        print(i)\n",
    "\n",
    "    col_1 = X_train.columns[i]\n",
    "\n",
    "    for col_2 in X_train.columns[i + 1:]:\n",
    "        if X_train[col_1].equals(X_train[col_2]):\n",
    "            duplicated_feat.append(col_2)\n",
    "            \n",
    "len(duplicated_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((483, 14), (207, 14))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicated features\n",
    "X_train.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "X_test.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I keep a copy of the dataset except constant and duplicated variables\n",
    "# to measure the performance of machine learning models\n",
    "# at the end of the notebook\n",
    "\n",
    "X_train_basic_filter = X_train.copy()\n",
    "X_test_basic_filter = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Remove correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlated features:  0\n"
     ]
    }
   ],
   "source": [
    "# find and remove correlated features\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "corr_features = correlation(X_train, 0.8)\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((483, 14), (207, 14))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed correlated  features\n",
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a copy of the dataset at  this stage\n",
    "X_train_corr = X_train.copy()\n",
    "X_test_corr = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Remove features using univariate ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find important features using univariate roc-auc\n",
    "\n",
    "# loop to build a tree, make predictions and get the roc-auc\n",
    "# for each feature of the train set\n",
    "\n",
    "roc_values = []\n",
    "for feature in X_train.columns:\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train[feature].fillna(0).to_frame(), y_train)\n",
    "    y_scored = clf.predict_proba(X_test[feature].fillna(0).to_frame())\n",
    "    roc_values.append(roc_auc_score(y_test, y_scored[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2744ca13470>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's add the variable names and order it for clearer visualisation\n",
    "roc_values = pd.Series(roc_values)\n",
    "roc_values.index = X_train.columns\n",
    "roc_values.sort_values(ascending=False).plot.bar(figsize=(8, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 14)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by removing features with univariate roc_auc == 0.5\n",
    "# we remove another 30 features\n",
    "\n",
    "selected_feat = roc_values[roc_values>0.5]\n",
    "len(selected_feat), X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Embedded methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear models benefit from feature scaling\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scaled_X_train = scaler.transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Select features by the regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features using the coefficient of a non\n",
    "# regularised logistic regression\n",
    "\n",
    "sel_ = SelectFromModel(LogisticRegression(C=1000))\n",
    "sel_.fit(scaled_X_train, y_train)\n",
    "\n",
    "# remove features with zero coefficient from dataset\n",
    "# and parse again as dataframe (output of sklearn is\n",
    "# numpy array)\n",
    "X_train_coef = pd.DataFrame(sel_.transform(X_train))\n",
    "X_test_coef = pd.DataFrame(sel_.transform(X_test))\n",
    "\n",
    "# add the columns name\n",
    "X_train_coef.columns = X_train.columns[(sel_.get_support())]\n",
    "X_test_coef.columns = X_train.columns[(sel_.get_support())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((483, 3), (207, 3))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_coef.shape, X_test_coef.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Select features by random forests derived importance \n",
    "\n",
    "Random forests are one the most popular machine learning algorithms. They are so successful because they provide in general a good predictive performance, low overfitting and easy interpretability. This interpretability is given by the fact that it is straightforward to derive the importance of each variable on the tree decision. In other words, it is easy to compute how much each variable is contributing to the decision.\n",
    "\n",
    "Random forests consist of 4-12 hundred decision trees, each of them built over a random extraction of the observations from the dataset and a random extraction of the features. Not every tree sees all the features or all the observations, and this guarantees that the trees are de-correlated and therefore less prone to over-fitting. Each tree is also a sequence of yes-no questions based on a single or combination of features. At each node (this is at each question), the three divides the dataset into 2 buckets, each of them hosting observations that are more similar among themselves and different from the ones in the other bucket. Therefore, the importance of each feature is derived by how \"pure\" each of the buckets is. \n",
    "\n",
    "For classification, the measure of impurity is either the Gini impurity or the information gain/entropy. For regression the  measure of impurity is variance. Therefore, when training a tree, it is possible to compute how much each feature decreases the impurity. The more a feature decreases the impurity, the more important the feature is. In random forests, the impurity decrease from each feature can be averaged across trees to determine the final importance of the variable.\n",
    "\n",
    "To give you a better intuition, features that are selected at the top of the trees are in general more important than features that are selected at the end nodes of the trees, as generally the top splits lead to bigger information gains.\n",
    "\n",
    "**Note**\n",
    "- Random Forests and decision trees in general give preference to features with high cardinality\n",
    "- Correlated features will be given equal or similar importance, but overall reduced importance compared to the same tree built without correlated counterparts.\n",
    "\n",
    "http://localhost:8888/notebooks/01-ML/02-General/FeatureSelection/09.2_Random_forest_importance.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features using the impotance derived from\n",
    "# random forests\n",
    "\n",
    "sel_ = SelectFromModel(RandomForestClassifier(n_estimators=400))\n",
    "sel_.fit(scaled_X_train, y_train)\n",
    "\n",
    "# remove features with zero coefficient from dataset\n",
    "# and parse again as dataframe (output of sklearn is\n",
    "# numpy array)\n",
    "X_train_rf = pd.DataFrame(sel_.transform(X_train))\n",
    "X_test_rf = pd.DataFrame(sel_.transform(X_test))\n",
    "\n",
    "# add the columns name\n",
    "X_train_rf.columns = X_train.columns[(sel_.get_support())]\n",
    "X_test_rf.columns = X_train.columns[(sel_.get_support())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((483, 7), (207, 7))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_rf.shape, X_test_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this command let's me visualise those features that were selected.\n",
    "\n",
    "# sklearn will select those features which importance values\n",
    "# are greater than the mean of all the coefficients.\n",
    "\n",
    "sel_.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's make a list and count the selected features\n",
    "randomforest_feature = X_train.columns[(sel_.get_support())]\n",
    "len(randomforest_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11',\n",
       "       'A12', 'A13', 'A14'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomforest_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-f8671976df55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# and now let's plot the distribution of importances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "# and now let's plot the distribution of importances\n",
    "\n",
    "pd.Series(sel_.estimator_.feature_importances_.ravel()).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 14\n",
      "selected features: 12\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-168-8f6e2a82f4c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'selected features: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselected_feat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m print('features with coefficients greater than the mean coefficient: {}'.format(\n\u001b[1;32m----> 9\u001b[1;33m     np.sum(sel_.estimator_.feature_importances_ > sel_.estimator_.feature_importances_.mean())))\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "# and now, let's compare the  amount of selected features\n",
    "# with the amount of features which importance is above the\n",
    "# mean importance, to make sure we understand the output of\n",
    "# sklearn\n",
    "\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients greater than the mean coefficient: {}'.format(\n",
    "    np.sum(sel_.estimator_.feature_importances_ > sel_.estimator_.feature_importances_.mean())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting features by using tree derived feature importance is a very srtaightforward, fast and generally accurate way of selecting good features for machine learning. In particular, if you are going to build tree methods.\n",
    "\n",
    "However, as I said, correlated features will show in a tree similar and lowered importance, compared to what their importance would be if the tree was built without correlated counterparts.\n",
    "\n",
    "In situations like this, it is better to select features recursively, rather than altogether like I am doing in this lecture.\n",
    "\n",
    "\n",
    "That is all for this lecture, I hope you enjoyed it and see you in the next one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Recursive feature selection using random forests importance\n",
    "\n",
    "Random Forests assign equal or similar importance to features that are highly correlated. In addition, when features are correlated, the importance assigned is lower than the importance attributed to the feature itself, should the tree be built without the correlated counterparts.\n",
    "\n",
    "Therefore, instead of eliminating features based  on importance by brute force like we did in the previous lecture, we may get a better selection by removing one feature at a time, and recalculating the importance on each round.\n",
    "\n",
    "This method is an hybrid between embedded and wrapper methods: it is based on computation derived when fitting the model, but it also requires fitting several models.\n",
    "\n",
    "The cycle is as follows:\n",
    "\n",
    "- Build random forests using all features\n",
    "- Remove least important feature\n",
    "- Build random forests and recalculate importance\n",
    "- Repeat until a criteria is met\n",
    "\n",
    "In this situation, when a feature that is highly correlated to another one is removed, then, the importance of the remaining feature increases. This may lead to a better subset feature space selection. On the downside, building several random forests is quite time consuming, in particular if the dataset contains a high number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFE(estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                     class_weight=None, criterion='gini',\n",
       "                                     max_depth=None, max_features='auto',\n",
       "                                     max_leaf_nodes=None, max_samples=None,\n",
       "                                     min_impurity_decrease=0.0,\n",
       "                                     min_impurity_split=None,\n",
       "                                     min_samples_leaf=1, min_samples_split=2,\n",
       "                                     min_weight_fraction_leaf=0.0,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     oob_score=False, random_state=None,\n",
       "                                     verbose=0, warm_start=False),\n",
       "    n_features_to_select=10, step=1, verbose=0)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first I specify the Random Forest instance, indicating the number of trees\n",
    "\n",
    "# Then I use the selectFromModel object from sklearn to automatically select the features\n",
    "\n",
    "# RFE will remove one feature at each iteration, the least  important.\n",
    "# then it will build another random forest and repeat till a criteria is met.\n",
    "\n",
    "# in sklearn the criteria to stop is an arbitrary number of features to select, that you need to decide before hand\n",
    "# not the best solution, but a solution\n",
    "\n",
    "sel_ = RFE(RandomForestClassifier(n_estimators=100), n_features_to_select=10)\n",
    "sel_.fit(std_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True, False, False,  True,  True])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this command let's me visualise those features that were selected.\n",
    "sel_.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's add the variable names and order it for clearer visualisation\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "len(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A2', 'A3', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A13', 'A14'], dtype='object')"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's display the list of features\n",
    "selected_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Gradient Boosted trees importance\n",
    "\n",
    "Similarly to selecting features using Random Forests derived feature importance, you can select features based on the importance derived by gradient boosted trees. And you can do that in one go, or in a recursive manner, depending on how much time you have, how many features are in the dataset, and whether they are correlated or not.\n",
    "\n",
    "I will demonstrate how to select features using Gradient boosted trees derived importance using sklearn on a classification problem, using the Paribas claims dataset from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=GradientBoostingClassifier(ccp_alpha=0.0,\n",
       "                                                     criterion='friedman_mse',\n",
       "                                                     init=None,\n",
       "                                                     learning_rate=0.1,\n",
       "                                                     loss='deviance',\n",
       "                                                     max_depth=3,\n",
       "                                                     max_features=None,\n",
       "                                                     max_leaf_nodes=None,\n",
       "                                                     min_impurity_decrease=0.0,\n",
       "                                                     min_impurity_split=None,\n",
       "                                                     min_samples_leaf=1,\n",
       "                                                     min_samples_split=2,\n",
       "                                                     min_weight_fraction_leaf=0.0,\n",
       "                                                     n_estimators=100,\n",
       "                                                     n_iter_no_change=None,\n",
       "                                                     presort='deprecated',\n",
       "                                                     random_state=None,\n",
       "                                                     subsample=1.0, tol=0.0001,\n",
       "                                                     validation_fraction=0.1,\n",
       "                                                     verbose=0,\n",
       "                                                     warm_start=False),\n",
       "                max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first I will select features all together in one go\n",
    "# by contemplating their importance after fitting only\n",
    "# 1 gradient boosted tree\n",
    "\n",
    "sel_ = SelectFromModel(GradientBoostingClassifier())\n",
    "sel_.fit(std_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's add the variable names and order it for clearer visualisation\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "len(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A8'], dtype='object')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFE(estimator=GradientBoostingClassifier(ccp_alpha=0.0,\n",
       "                                         criterion='friedman_mse', init=None,\n",
       "                                         learning_rate=0.1, loss='deviance',\n",
       "                                         max_depth=3, max_features=None,\n",
       "                                         max_leaf_nodes=None,\n",
       "                                         min_impurity_decrease=0.0,\n",
       "                                         min_impurity_split=None,\n",
       "                                         min_samples_leaf=1,\n",
       "                                         min_samples_split=2,\n",
       "                                         min_weight_fraction_leaf=0.0,\n",
       "                                         n_estimators=100,\n",
       "                                         n_iter_no_change=None,\n",
       "                                         presort='deprecated',\n",
       "                                         random_state=None, subsample=1.0,\n",
       "                                         tol=0.0001, validation_fraction=0.1,\n",
       "                                         verbose=0, warm_start=False),\n",
       "    n_features_to_select=1, step=1, verbose=0)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next I will select features recursively for comparison\n",
    "\n",
    "sel_ = RFE(GradientBoostingClassifier(), n_features_to_select=len(selected_feat))\n",
    "sel_.fit(std_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's add the variable names and order it for clearer visualisation\n",
    "selected_feat_rfe = X_train.columns[(sel_.get_support())]\n",
    "len(selected_feat_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A8'], dtype='object')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat_rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Lasso Regularization\n",
    "\n",
    "Regularization consists in adding a penalty to the different parameters of the machine learning model to reduce the freedom of the model and in other words to avoid overfitting. In linear model regularization, the penalty is applied over the coefficients that multiply each of the predictors. From the different types of regularization, Lasso or L1 has the property that is able to shrink some of the coefficients to zero. Therefore, that feature can be removed from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=1, class_weight=None, dual=False,\n",
       "                                             fit_intercept=True,\n",
       "                                             intercept_scaling=1, l1_ratio=None,\n",
       "                                             max_iter=100, multi_class='auto',\n",
       "                                             n_jobs=None, penalty='l1',\n",
       "                                             random_state=None,\n",
       "                                             solver='liblinear', tol=0.0001,\n",
       "                                             verbose=0, warm_start=False),\n",
       "                max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, specify the Logistic Regression model and selection the Lasso (L1) penalty\n",
    "sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1',solver='liblinear'))\n",
    "\n",
    "# Second, use the selectFromModel object from sklearn, which will select\n",
    "# in theory the features which coefficients are non-zero\n",
    "sel_.fit(std_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize those features that were kept\n",
    "sel_.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 14\n",
      "selected features: 14\n",
      "features with coefficients shrank to zero: 0\n"
     ]
    }
   ],
   "source": [
    "# Make a list with the selected features\n",
    "\n",
    "lasso_features = X_train.columns[(sel_.get_support())]\n",
    "\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(lasso_features)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "    np.sum(sel_.estimator_.coef_ == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can identify the removed features like this:\n",
    "removed_feats = X_train.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\n",
    "removed_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11',\n",
       "       'A12', 'A13', 'A14'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2744cab51d0>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of coefficients\n",
    "pd.Series(sel_.estimator_.coef_.ravel()).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, some coefficients are positive and some are negative, suggesting that some features are negatively associated with the outcome (the more of the feature the less of the outcome) and viceversa.\n",
    "\n",
    "However, the absolute value of the coefficients inform about the importance of the feature on the outcome, and not the sign. Therefore, the feature selection is done filtering on absolute values of coefficients. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47701051608056694"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the feature importance is informed by the absolute value of\n",
    "# the coefficient, and not the sign.\n",
    "# therefore, let's recalculate the mean using the absolute values instead\n",
    "\n",
    "np.abs(sel_.estimator_.coef_).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2744cb05b38>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# and now let's plot the histogram of absolute coefficients\n",
    "\n",
    "pd.Series(np.abs(sel_.estimator_.coef_).ravel()).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 14\n",
      "selected features: 14\n",
      "features with coefficients greater than the mean coefficient: 4\n"
     ]
    }
   ],
   "source": [
    "# and now, let's compare the  amount of selected features\n",
    "# with the amount of features which coefficient is above the\n",
    "# mean coefficient, to make sure we understand the output of\n",
    "# sklearn\n",
    "\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(lasso_features)))\n",
    "print('features with coefficients greater than the mean coefficient: {}'.format(\n",
    "    np.sum(np.abs(sel_.estimator_.coef_) > np.abs(sel_.estimator_.coef_).mean())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Regression coefficients are affected by regularisation\n",
    "\n",
    "As I mentioned previously, regularisation applies a penalty on the coefficients, in order to reduce their influence and create models that generalise better. This is very good to improve  model performance. However, it masks the true relationship between the predictor X and the outcome Y. Let's explore why.\n",
    "\n",
    "http://localhost:8888/notebooks/01-ML/02-General/FeatureSelection/08.3_Regression_coefficients_and_regularisation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I build 4 different models, from a highly regularised one\n",
    "# to a non regularised one (C=1000)\n",
    "coefs_df = []\n",
    "for c in [1, 10, 100, 1000]:\n",
    "    logit = LogisticRegression(C=c, penalty='l2')\n",
    "    logit.fit(scaler.transform(X_train.fillna(0)), y_train)\n",
    "    \n",
    "    # store the coefficients of the variables in a list\n",
    "    coefs_df.append(pd.Series(logit.coef_.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A1</th>\n",
       "      <td>-0.033460</td>\n",
       "      <td>-0.044539</td>\n",
       "      <td>-0.047205</td>\n",
       "      <td>-0.047514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2</th>\n",
       "      <td>-0.267938</td>\n",
       "      <td>-0.314190</td>\n",
       "      <td>-0.322951</td>\n",
       "      <td>-0.323939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3</th>\n",
       "      <td>-0.156090</td>\n",
       "      <td>-0.190003</td>\n",
       "      <td>-0.196532</td>\n",
       "      <td>-0.197273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A4</th>\n",
       "      <td>0.300837</td>\n",
       "      <td>0.312363</td>\n",
       "      <td>0.313613</td>\n",
       "      <td>0.313741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A5</th>\n",
       "      <td>0.826030</td>\n",
       "      <td>0.864826</td>\n",
       "      <td>0.871259</td>\n",
       "      <td>0.872000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        1         10        100       1000\n",
       "A1 -0.033460 -0.044539 -0.047205 -0.047514\n",
       "A2 -0.267938 -0.314190 -0.322951 -0.323939\n",
       "A3 -0.156090 -0.190003 -0.196532 -0.197273\n",
       "A4  0.300837  0.312363  0.313613  0.313741\n",
       "A5  0.826030  0.864826  0.871259  0.872000"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now I create a dataframe with the coefficients for all\n",
    "# the variables for the 4 different logistic regression models\n",
    "\n",
    "coefs = pd.concat(coefs_df, axis=1)\n",
    "coefs.columns = [1, 10, 100, 1000]\n",
    "coefs.index = X_train.columns\n",
    "coefs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.000000</th>\n",
       "      <th>2.302585</th>\n",
       "      <th>4.605170</th>\n",
       "      <th>6.907755</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A1</th>\n",
       "      <td>-0.033460</td>\n",
       "      <td>-0.044539</td>\n",
       "      <td>-0.047205</td>\n",
       "      <td>-0.047514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2</th>\n",
       "      <td>-0.267938</td>\n",
       "      <td>-0.314190</td>\n",
       "      <td>-0.322951</td>\n",
       "      <td>-0.323939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3</th>\n",
       "      <td>-0.156090</td>\n",
       "      <td>-0.190003</td>\n",
       "      <td>-0.196532</td>\n",
       "      <td>-0.197273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A4</th>\n",
       "      <td>0.300837</td>\n",
       "      <td>0.312363</td>\n",
       "      <td>0.313613</td>\n",
       "      <td>0.313741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A5</th>\n",
       "      <td>0.826030</td>\n",
       "      <td>0.864826</td>\n",
       "      <td>0.871259</td>\n",
       "      <td>0.872000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0.000000  2.302585  4.605170  6.907755\n",
       "A1 -0.033460 -0.044539 -0.047205 -0.047514\n",
       "A2 -0.267938 -0.314190 -0.322951 -0.323939\n",
       "A3 -0.156090 -0.190003 -0.196532 -0.197273\n",
       "A4  0.300837  0.312363  0.313613  0.313741\n",
       "A5  0.826030  0.864826  0.871259  0.872000"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.columns = np.log([1, 10, 100, 1000])\n",
    "coefs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2744cb56dd8>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the change in coefficients with the penalty\n",
    "coefs.T.plot(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_logit = LogisticRegression(C=1, penalty='l2')\n",
    "l1_logit.fit(scaler.transform(X_train.fillna(0)), y_train)\n",
    "\n",
    "# I count the number of coefficients with zero values\n",
    "# and it is zero, as expected\n",
    "np.sum(l1_logit.coef_ == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Recursive Feature Elimination\n",
    "\n",
    "http://localhost:8888/notebooks/01-ML/02-General/FeatureSelection/11.02_Hybrid_Recursive_feature_elimination.ipynb\n",
    "\n",
    "This method consists of the following steps:\n",
    "\n",
    "1) Rank the features according to their importance derived from a machine learning algorithm: it can be tree importance, or LASSO / Ridge, or the linear / logistic regression coefficients.\n",
    "\n",
    "2) Remove one feature -the least important- and build a machine learning algorithm utilising the remaining features.\n",
    "\n",
    "3) Calculate a performance metric of your choice: roc-auc, mse, rmse, accuracy.\n",
    "\n",
    "4) If the metric decreases by more of an arbitrarily set threshold, then that feature is important and should be kept. Otherwise, we can remove that feature.\n",
    "\n",
    "5) Repeat steps 2-4 until all features have been removed (and therefore evaluated) and the drop in performance assessed.\n",
    "\n",
    "\n",
    "I call this a hybrid method because:\n",
    "\n",
    "- it combines the importance derived from the machine learning algorithm like embedded methods,\n",
    "- and it removes as well one feature at a time, and calculates a new metric based on the new subset of features and the machine learning algorithm of choice, like wrapper methods.\n",
    "\n",
    "The difference between this method and the step backwards feature selection we learned in previous lectures lies in that it does not remove all features first in order to determine which one to remove. It removes the least important one, based on the machine learning model derived important. And then, it makes an assessment as to whether that feature should be removed or not. So it removes each feature only once during selection, whereas step backward feature selection removes all the features at each step of selection.\n",
    "\n",
    "This method is therefore faster than wrapper methods and generally better than embedded methods. In practice it works extremely well. It does also account for correlations (depending on how stringent you set the arbitrary performance drop threshold). On the downside, the drop in performance assessed to decide whether the feature should be kept or removed, is set arbitrarily. The smaller the drop the more features will be selected, and vice versa.\n",
    "\n",
    "I will demonstrate how to select features using this method on a regression and classification problem. For classification I will use the Paribas claims dataset from Kaggle. For regression, the House Price dataset from Kaggle.\n",
    "\n",
    "**Note** For the demonstration, I will use XGBoost, but this method is useful for any machine learning algorithm. In fact, the importance of the features are determined specifically for the algorithm used. Therefore, different algorithms may return different subsets of important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test all features xgb ROC AUC=0.925824\n"
     ]
    }
   ],
   "source": [
    "# the first step of this procedure  consists in building\n",
    "# a machine learning algorithm using all the available features\n",
    "# and then determine the importance of the features according\n",
    "# to the algorithm\n",
    "\n",
    "# set the seed for reproducibility\n",
    "seed_val = 1000000000\n",
    "np.random.seed(seed_val)\n",
    "\n",
    "# build initial model using all the features\n",
    "model_all_features = xgb.XGBClassifier(\n",
    "    nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "model_all_features.fit(X_train, y_train)\n",
    "\n",
    "# calculate the roc-auc in the test set\n",
    "y_pred_test = model_all_features.predict_proba(X_test)[:, 1]\n",
    "auc_score_all = roc_auc_score(y_test, y_pred_test)\n",
    "print('Test all features xgb ROC AUC=%f' % (auc_score_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2744ce6e748>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the second step consist of deriving the importance of \n",
    "# each feature and ranking them from the least to the most\n",
    "# important\n",
    "\n",
    "# get feature name and importance\n",
    "features = pd.Series(model_all_features.feature_importances_)\n",
    "features.index = X_train.columns\n",
    "\n",
    "# sort the features by importance\n",
    "features.sort_values(ascending=True, inplace=True)\n",
    "\n",
    "# plot\n",
    "features.plot.bar(figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A11', 'A2', 'A1', 'A3', 'A4', 'A6', 'A7', 'A13', 'A10', 'A5', 'A14', 'A12', 'A9', 'A8']\n"
     ]
    }
   ],
   "source": [
    "# view the list of ordered features\n",
    "features = list(features.index)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing recursive feature elimination\n",
      "\n",
      "testing feature:  A11  which is feature  1  out of  14\n",
      "New Test ROC AUC=0.9279184917158636\n",
      "All features Test ROC AUC=0.9258236526375928\n",
      "Drop in ROC AUC=-0.0020948390782707937\n",
      "remove:  A11\n",
      "\n",
      "testing feature:  A2  which is feature  2  out of  14\n",
      "New Test ROC AUC=0.930917920396115\n",
      "All features Test ROC AUC=0.9279184917158636\n",
      "Drop in ROC AUC=-0.0029994286802513814\n",
      "remove:  A2\n",
      "\n",
      "testing feature:  A1  which is feature  3  out of  14\n",
      "New Test ROC AUC=0.9314416301656827\n",
      "All features Test ROC AUC=0.930917920396115\n",
      "Drop in ROC AUC=-0.0005237097695677262\n",
      "remove:  A1\n",
      "\n",
      "testing feature:  A3  which is feature  4  out of  14\n",
      "New Test ROC AUC=0.9220148543134641\n",
      "All features Test ROC AUC=0.9314416301656827\n",
      "Drop in ROC AUC=0.009426775852218627\n",
      "keep:  A3\n",
      "\n",
      "testing feature:  A4  which is feature  5  out of  14\n",
      "New Test ROC AUC=0.9247762330984575\n",
      "All features Test ROC AUC=0.9314416301656827\n",
      "Drop in ROC AUC=0.006665397067225243\n",
      "keep:  A4\n",
      "\n",
      "testing feature:  A6  which is feature  6  out of  14\n",
      "New Test ROC AUC=0.929822890877928\n",
      "All features Test ROC AUC=0.9314416301656827\n",
      "Drop in ROC AUC=0.0016187392877546891\n",
      "keep:  A6\n",
      "\n",
      "testing feature:  A7  which is feature  7  out of  14\n",
      "New Test ROC AUC=0.9286802513806893\n",
      "All features Test ROC AUC=0.9314416301656827\n",
      "Drop in ROC AUC=0.0027613787849933846\n",
      "keep:  A7\n",
      "\n",
      "testing feature:  A13  which is feature  8  out of  14\n",
      "New Test ROC AUC=0.9196343553608836\n",
      "All features Test ROC AUC=0.9314416301656827\n",
      "Drop in ROC AUC=0.01180727480479915\n",
      "keep:  A13\n",
      "\n",
      "testing feature:  A10  which is feature  9  out of  14\n",
      "New Test ROC AUC=0.9281089316320702\n",
      "All features Test ROC AUC=0.9314416301656827\n",
      "Drop in ROC AUC=0.0033326985336125103\n",
      "keep:  A10\n",
      "\n",
      "testing feature:  A5  which is feature  10  out of  14\n",
      "New Test ROC AUC=0.9266806322605219\n",
      "All features Test ROC AUC=0.9314416301656827\n",
      "Drop in ROC AUC=0.004760997905160824\n",
      "keep:  A5\n",
      "\n",
      "testing feature:  A14  which is feature  11  out of  14\n",
      "New Test ROC AUC=0.9246810131403543\n",
      "All features Test ROC AUC=0.9314416301656827\n",
      "Drop in ROC AUC=0.006760617025328375\n",
      "keep:  A14\n",
      "\n",
      "testing feature:  A12  which is feature  12  out of  14\n",
      "New Test ROC AUC=0.9331555894115406\n",
      "All features Test ROC AUC=0.9314416301656827\n",
      "Drop in ROC AUC=-0.0017139592458579322\n",
      "remove:  A12\n",
      "\n",
      "testing feature:  A9  which is feature  13  out of  14\n",
      "New Test ROC AUC=0.9331555894115406\n",
      "All features Test ROC AUC=0.9331555894115406\n",
      "Drop in ROC AUC=0.0\n",
      "remove:  A9\n",
      "\n",
      "testing feature:  A8  which is feature  14  out of  14\n",
      "New Test ROC AUC=0.8665016187392878\n",
      "All features Test ROC AUC=0.9331555894115406\n",
      "Drop in ROC AUC=0.06665397067225287\n",
      "keep:  A8\n",
      "DONE!!\n",
      "total features to remove:  5\n",
      "total features to keep:  9\n"
     ]
    }
   ],
   "source": [
    "# the final step consists in removing one at a time\n",
    "# all the features, from the least to the most\n",
    "# important, and build an xgboost at each round.\n",
    "\n",
    "# once we build the model, we calculate the new roc-auc\n",
    "# if the new roc-auc is smaller than the original one\n",
    "# (with all the features), then that feature that was removed\n",
    "# was important, and we should keep it.\n",
    "# otherwise, we should remove the feature\n",
    "\n",
    "# recursive feature elimination:\n",
    "\n",
    "# first we arbitrarily set the drop in roc-auc\n",
    "# if the drop is below this threshold,\n",
    "# the feature will be removed\n",
    "tol = 0.0005\n",
    "\n",
    "print('doing recursive feature elimination')\n",
    "\n",
    "# we initialise a list where we will collect the\n",
    "# features we should remove\n",
    "features_to_remove = []\n",
    "\n",
    "# set a counter to know how far ahead the loop is going\n",
    "count = 1\n",
    "\n",
    "# now we loop over all the features, in order of importance:\n",
    "# remember that features is the list of ordered features\n",
    "# by importance\n",
    "for feature in features:\n",
    "    print()\n",
    "    print('testing feature: ', feature, ' which is feature ', count,\n",
    "          ' out of ', len(features))\n",
    "    count = count + 1\n",
    "\n",
    "    # initialise model\n",
    "    model_int = xgb.XGBClassifier(\n",
    "        nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "    # fit model with all variables minus the removed features\n",
    "    # and the feature to be evaluated\n",
    "    model_int.fit(\n",
    "        X_train.drop(features_to_remove + [feature], axis=1), y_train)\n",
    "\n",
    "    # make a prediction over the test set\n",
    "    y_pred_test = model_int.predict_proba(\n",
    "        X_test.drop(features_to_remove + [feature], axis=1))[:, 1]\n",
    "\n",
    "    # calculate the new roc-auc\n",
    "    auc_score_int = roc_auc_score(y_test, y_pred_test)\n",
    "    print('New Test ROC AUC={}'.format((auc_score_int)))\n",
    "\n",
    "    # print the original roc-auc with all the features\n",
    "    print('All features Test ROC AUC={}'.format((auc_score_all)))\n",
    "\n",
    "    # determine the drop in the roc-auc\n",
    "    diff_auc = auc_score_all - auc_score_int\n",
    "\n",
    "    # compare the drop in roc-auc with the tolerance\n",
    "    # we set previously\n",
    "    if diff_auc >= tol:\n",
    "        print('Drop in ROC AUC={}'.format(diff_auc))\n",
    "        print('keep: ', feature)\n",
    "        print\n",
    "    else:\n",
    "        print('Drop in ROC AUC={}'.format(diff_auc))\n",
    "        print('remove: ', feature)\n",
    "        print\n",
    "        # if the drop in the roc is small and we remove the\n",
    "        # feature, we need to set the new roc to the one based on\n",
    "        # the remaining features\n",
    "        auc_score_all = auc_score_int\n",
    "        \n",
    "        # and append the feature to remove to the collecting\n",
    "        # list\n",
    "        features_to_remove.append(feature)\n",
    "\n",
    "# now the loop is finished, we evaluated all the features\n",
    "print('DONE!!')\n",
    "print('total features to remove: ', len(features_to_remove))\n",
    "\n",
    "# determine the features to keep (those we won't remove)\n",
    "features_to_keep = [x for x in features if x not in features_to_remove]\n",
    "print('total features to keep: ', len(features_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test selected features ROC AUC=0.933156\n",
      "Test all features ROC AUC=0.933156\n"
     ]
    }
   ],
   "source": [
    "# capture the 56 selected features\n",
    "seed_val = 1000000000\n",
    "np.random.seed(seed_val)\n",
    "\n",
    "# build initial model\n",
    "final_xgb = xgb.XGBClassifier(\n",
    "    nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "# fit the model with the selected features\n",
    "final_xgb.fit(X_train[features_to_keep], y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred_test = final_xgb.predict_proba(X_test[features_to_keep])[:, 1]\n",
    "\n",
    "# calculate roc-auc\n",
    "auc_score_final = roc_auc_score(y_test, y_pred_test)\n",
    "print('Test selected features ROC AUC=%f' % (auc_score_final))\n",
    "print('Test all features ROC AUC=%f' % (auc_score_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the xgb model built with 56 features shows a similar performance than the one built with the total features (0.715 vs 0.713).\n",
    "\n",
    "We may not be able to get this right from the beginning though, as we did here. This method of feature selection does require that you try a few different tolerances / thresholds until you find the right number of features.\n",
    "\n",
    "Why don't you go ahead and try different values? Try with lower and bigger thresholds and get a feeling of how much this affects the number of selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Recursive Feature Addition\n",
    "\n",
    "http://localhost:8888/notebooks/01-ML/02-General/FeatureSelection/11.03_Hybrid_Recursive_feature_addition.ipynb\n",
    "\n",
    "This method consists of the following steps:\n",
    "\n",
    "1) Rank the features according to their importance derived from a machine learning algorithm: it can be tree importance, or LASSO / Ridge, or the linear / logistic regression coefficients.\n",
    "\n",
    "2) Build a machine learning model with only 1 feature, the most important one, and calculate the model metric for performance.\n",
    "\n",
    "3) Add one feature -the most important- and build a machine learning algorithm utilising the added and any feature from previous rounds.\n",
    "\n",
    "4) Calculate a performance metric of your choice: roc-auc, mse, rmse, accuracy.\n",
    "\n",
    "5) If the metric increases by more than an arbitrarily set threshold, then that feature is important and should be kept. Otherwise, we can remove that feature.\n",
    "\n",
    "6) Repeat steps 2-5 until all features have been removed (and therefore evaluated) and the drop in performance assessed.\n",
    "\n",
    "\n",
    "I call this a hybrid method because:\n",
    "\n",
    "- it combines the importance derived from the machine learning algorithm like embedded methods,\n",
    "- and it adds as well one feature at a time, and calculates a new metric based on the new subset of features and the machine learning algorithm of choice, like wrapper methods.\n",
    "\n",
    "The difference between this method and the step forward feature selection we learned in previous lectures lies in that it does not add all possible features first, in order to determine which one to keep. It adds the most important one, based on the machine learning model derived important. And then, it makes an assessment as to whether that feature should be kept or not. And then it moves to the next feature.\n",
    "\n",
    "This method is therefore faster than wrapper methods and generally better than embedded methods. In practice it works extremely well. It does also account for correlations (depending on how stringent you set the arbitrary performance drop threshold). On the downside, the increase in performance assessed to decide whether the feature should be kept or removed, is set arbitrarily. The smaller the increase the more features will be selected, and vice versa.\n",
    "\n",
    "I will demonstrate how to select features using this method on a regression and classification problem. For classification I will use the Paribas claims dataset from Kaggle. For regression, the House Price dataset from Kaggle.\n",
    "\n",
    "**Note** For the demonstration, I will use XGBoost, but this method is useful for any machine learning algorithm. In fact, the importance of the features are determined specifically for the algorithm used. Therefore, different algorithms may return different subsets of important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test all features xgb ROC AUC=0.925824\n"
     ]
    }
   ],
   "source": [
    "# the first step of this procedure  consists in building\n",
    "# a machine learning algorithm using all the available features\n",
    "# and then determine the importance of the features according\n",
    "# to the algorithm\n",
    "\n",
    "# set the seed for reproducibility\n",
    "seed_val = 1000000000\n",
    "np.random.seed(seed_val)\n",
    "\n",
    "# build initial model using all the features\n",
    "model_all_features = xgb.XGBClassifier(\n",
    "    nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "model_all_features.fit(X_train, y_train)\n",
    "\n",
    "# calculate the roc-auc in the test set\n",
    "y_pred_test = model_all_features.predict_proba(X_test)[:, 1]\n",
    "auc_score_all = roc_auc_score(y_test, y_pred_test)\n",
    "print('Test all features xgb ROC AUC=%f' % (auc_score_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2744cf0d470>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the second step consist of deriving the importance of \n",
    "# each feature and ranking them from the most to the least\n",
    "# important\n",
    "\n",
    "# get feature name and importance\n",
    "features = pd.Series(model_all_features.feature_importances_)\n",
    "features.index = X_train.columns\n",
    "\n",
    "# sort the features by importance\n",
    "features.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "# plot\n",
    "features.plot.bar(figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A8', 'A9', 'A12', 'A14', 'A5', 'A10', 'A13', 'A7', 'A6', 'A4', 'A3', 'A1', 'A2', 'A11']\n"
     ]
    }
   ],
   "source": [
    "# view the list of ordered features\n",
    "features = list(features.index)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test one feature xgb ROC AUC=0.870310\n"
     ]
    }
   ],
   "source": [
    "# next, we need to build a machine learning\n",
    "# algorithm using only the most important feature\n",
    "\n",
    "# set the seed for reproducibility\n",
    "seed_val = 1000000000\n",
    "np.random.seed(seed_val)\n",
    "\n",
    "# build initial model using all the features\n",
    "model_one_feature = xgb.XGBClassifier(\n",
    "    nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "# train using only the most important feature\n",
    "model_one_feature.fit(X_train[features[0]].to_frame(), y_train)\n",
    "\n",
    "# calculate the roc-auc in the test set\n",
    "y_pred_test = model_one_feature.predict_proba(X_test[features[0]].to_frame())[:, 1]\n",
    "auc_score_first = roc_auc_score(y_test, y_pred_test)\n",
    "print('Test one feature xgb ROC AUC=%f' % (auc_score_first))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing recursive feature addition\n",
      "\n",
      "testing feature:  A9  which is feature  1  out of  14\n",
      "New Test ROC AUC=0.8955437059607694\n",
      "All features Test ROC AUC=0.8703104170634165\n",
      "Increase in ROC AUC=0.02523328889735288\n",
      "keep:  A9\n",
      "\n",
      "testing feature:  A12  which is feature  2  out of  14\n",
      "New Test ROC AUC=0.9086840601790135\n",
      "All features Test ROC AUC=0.8955437059607694\n",
      "Increase in ROC AUC=0.01314035421824411\n",
      "keep:  A12\n",
      "\n",
      "testing feature:  A14  which is feature  3  out of  14\n",
      "New Test ROC AUC=0.9077794705770329\n",
      "All features Test ROC AUC=0.9086840601790135\n",
      "Increase in ROC AUC=-0.0009045896019805877\n",
      "remove:  A14\n",
      "\n",
      "testing feature:  A5  which is feature  4  out of  14\n",
      "New Test ROC AUC=0.9078270805560845\n",
      "All features Test ROC AUC=0.9086840601790135\n",
      "Increase in ROC AUC=-0.0008569796229289661\n",
      "remove:  A5\n",
      "\n",
      "testing feature:  A10  which is feature  5  out of  14\n",
      "New Test ROC AUC=0.9146829175395164\n",
      "All features Test ROC AUC=0.9086840601790135\n",
      "Increase in ROC AUC=0.005998857360502874\n",
      "keep:  A10\n",
      "\n",
      "testing feature:  A13  which is feature  6  out of  14\n",
      "New Test ROC AUC=0.8973528851647304\n",
      "All features Test ROC AUC=0.9146829175395164\n",
      "Increase in ROC AUC=-0.01733003237478592\n",
      "remove:  A13\n",
      "\n",
      "testing feature:  A7  which is feature  7  out of  14\n",
      "New Test ROC AUC=0.8850219005903637\n",
      "All features Test ROC AUC=0.9146829175395164\n",
      "Increase in ROC AUC=-0.029661016949152685\n",
      "remove:  A7\n",
      "\n",
      "testing feature:  A6  which is feature  8  out of  14\n",
      "New Test ROC AUC=0.8937345267568082\n",
      "All features Test ROC AUC=0.9146829175395164\n",
      "Increase in ROC AUC=-0.02094839078270816\n",
      "remove:  A6\n",
      "\n",
      "testing feature:  A4  which is feature  9  out of  14\n",
      "New Test ROC AUC=0.9223481241668254\n",
      "All features Test ROC AUC=0.9146829175395164\n",
      "Increase in ROC AUC=0.0076652066273090735\n",
      "keep:  A4\n",
      "\n",
      "testing feature:  A3  which is feature  10  out of  14\n",
      "New Test ROC AUC=0.9108265092363358\n",
      "All features Test ROC AUC=0.9223481241668254\n",
      "Increase in ROC AUC=-0.011521614930489643\n",
      "remove:  A3\n",
      "\n",
      "testing feature:  A1  which is feature  11  out of  14\n",
      "New Test ROC AUC=0.9155398971624452\n",
      "All features Test ROC AUC=0.9223481241668254\n",
      "Increase in ROC AUC=-0.0068082270043802184\n",
      "remove:  A1\n",
      "\n",
      "testing feature:  A2  which is feature  12  out of  14\n",
      "New Test ROC AUC=0.892068177490002\n",
      "All features Test ROC AUC=0.9223481241668254\n",
      "Increase in ROC AUC=-0.030279946676823433\n",
      "remove:  A2\n",
      "\n",
      "testing feature:  A11  which is feature  13  out of  14\n",
      "New Test ROC AUC=0.9218720243763092\n",
      "All features Test ROC AUC=0.9223481241668254\n",
      "Increase in ROC AUC=-0.00047609979051621565\n",
      "remove:  A11\n",
      "DONE!!\n",
      "total features to keep:  5\n"
     ]
    }
   ],
   "source": [
    "# the final step consists in adding one at a time\n",
    "# all the features, from the most to the least\n",
    "# important, and build an xgboost at each round.\n",
    "\n",
    "# once we build the model, we calculate the new roc-auc\n",
    "# if the new roc-auc is bigger than the original one\n",
    "# (with one feature), then that feature that was added\n",
    "# was important, and we should keep it.\n",
    "# otherwise, we should remove the feature\n",
    "\n",
    "# recursive feature addition:\n",
    "\n",
    "# first we arbitrarily set the increase in roc-auc\n",
    "# if the increase is above this threshold,\n",
    "# the feature will be kept\n",
    "tol = 0.001\n",
    "\n",
    "print('doing recursive feature addition')\n",
    "\n",
    "# we initialise a list where we will collect the\n",
    "# features we should keep\n",
    "features_to_keep = [features[0]]\n",
    "\n",
    "# set a counter to know how far ahead the loop is going\n",
    "count = 1\n",
    "\n",
    "# now we loop over all the features, in order of importance:\n",
    "# remember that features is the list of ordered features\n",
    "# by importance\n",
    "for feature in features[1:]:\n",
    "    print()\n",
    "    print('testing feature: ', feature, ' which is feature ', count,\n",
    "          ' out of ', len(features))\n",
    "    count = count + 1\n",
    "\n",
    "    # initialise model\n",
    "    model_int = xgb.XGBClassifier(\n",
    "        nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "    # fit model with the selected features\n",
    "    # and the feature to be evaluated\n",
    "    model_int.fit(\n",
    "        X_train[features_to_keep + [feature] ], y_train)\n",
    "\n",
    "    # make a prediction over the test set\n",
    "    y_pred_test = model_int.predict_proba(\n",
    "        X_test[features_to_keep + [feature] ])[:, 1]\n",
    "\n",
    "    # calculate the new roc-auc\n",
    "    auc_score_int = roc_auc_score(y_test, y_pred_test)\n",
    "    print('New Test ROC AUC={}'.format((auc_score_int)))\n",
    "\n",
    "    # print the original roc-auc with one feature\n",
    "    print('All features Test ROC AUC={}'.format((auc_score_first)))\n",
    "\n",
    "    # determine the increase in the roc-auc\n",
    "    diff_auc = auc_score_int - auc_score_first\n",
    "\n",
    "    # compare the increase in roc-auc with the tolerance\n",
    "    # we set previously\n",
    "    if diff_auc >= tol:\n",
    "        print('Increase in ROC AUC={}'.format(diff_auc))\n",
    "        print('keep: ', feature)\n",
    "        print\n",
    "        # if the increase in the roc is bigger than the threshold\n",
    "        # we keep the feature and re-adjust the roc-auc to the new value\n",
    "        # considering the added feature\n",
    "        auc_score_first = auc_score_int\n",
    "        \n",
    "        # and we append the feature to keep to the list\n",
    "        features_to_keep.append(feature)\n",
    "    else:\n",
    "        # we ignore the feature\n",
    "        print('Increase in ROC AUC={}'.format(diff_auc))\n",
    "        print('remove: ', feature)\n",
    "        print\n",
    "\n",
    "\n",
    "# now the loop is finished, we evaluated all the features\n",
    "print('DONE!!')\n",
    "print('total features to keep: ', len(features_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test selected features ROC AUC=0.922348\n"
     ]
    }
   ],
   "source": [
    "# capture the 8 selected features\n",
    "seed_val = 1000000000\n",
    "np.random.seed(seed_val)\n",
    "\n",
    "# build initial model\n",
    "final_xgb = xgb.XGBClassifier(\n",
    "    nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "# fit the model with the selected features\n",
    "final_xgb.fit(X_train[features_to_keep], y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred_test = final_xgb.predict_proba(X_test[features_to_keep])[:, 1]\n",
    "\n",
    "# calculate roc-auc\n",
    "auc_score_final = roc_auc_score(y_test, y_pred_test)\n",
    "print('Test selected features ROC AUC=%f' % (auc_score_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the performance in Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to build random forests and compare performance in train and test set\n",
    "\n",
    "def run_randomForests(X_train, X_test, y_train, y_test):\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Train set')\n",
    "    pred = rf.predict_proba(X_train)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    print('Test set')\n",
    "    pred = rf.predict_proba(X_test)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9677687380993596\n",
      "Test set\n",
      "Random Forests roc-auc: 0.9318225099980956\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "run_randomForests(X_train_original,X_test_original,y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9677687380993596\n",
      "Test set\n",
      "Random Forests roc-auc: 0.9318225099980956\n"
     ]
    }
   ],
   "source": [
    "# filter methods - basic\n",
    "run_randomForests(X_train_basic_filter,X_test_basic_filter,y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9677687380993596\n",
      "Test set\n",
      "Random Forests roc-auc: 0.9318225099980956\n"
     ]
    }
   ],
   "source": [
    "# filter methods - correlation\n",
    "run_randomForests(X_train_corr,X_test_corr,y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9678899082568807\n",
      "Test set\n",
      "Random Forests roc-auc: 0.9292991811083603\n"
     ]
    }
   ],
   "source": [
    "# filter methods - univariate roc-auc\n",
    "run_randomForests(X_train[selected_feat.index],X_test_corr[selected_feat.index],y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9487450233685303\n",
      "Test set\n",
      "Random Forests roc-auc: 0.9175395162826128\n"
     ]
    }
   ],
   "source": [
    "# embedded methods - Logistic regression coefficients\n",
    "run_randomForests(X_train_coef,\n",
    "                  X_test_coef,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting features using the logistic regression coefficients gives a slightly worse performance than the univariate roc-auc (0.798 vs 0.795). However, keep in mind that the univariate model is using 90 features vs the only 28 used by the coefficients selection method. This means that many of those 90 features are still redundant, and we could potentially reduce the feature space further without loosing performance significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.966920546996711\n",
      "Test set\n",
      "Random Forests roc-auc: 0.9351552085317082\n"
     ]
    }
   ],
   "source": [
    "# embedded methods - Random forests\n",
    "run_randomForests(X_train_rf,\n",
    "                  X_test_rf,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well this was a big jump!! We can see how by selecting features using random forest importance, we reduce the feature space up to only 16 features, and yet the algorithm shows a greater predictive performance compared to the one using more features (0.813 vs 0.798)!!!\n",
    "\n",
    "We can see the power of feature selection now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to build logistic regression and compare performance in train and test set\n",
    "\n",
    "def run_logistic(X_train, X_test, y_train, y_test):\n",
    "    # function to train and test the performance of logistic regression\n",
    "    logit = LogisticRegression(random_state=44)\n",
    "    logit.fit(X_train, y_train)\n",
    "    print('Train set')\n",
    "    pred = logit.predict_proba(X_train)\n",
    "    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    print('Test set')\n",
    "    pred = logit.predict_proba(X_test)\n",
    "    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.9466678206681669\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.910207579508665\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "scaler = StandardScaler().fit(X_train_original)\n",
    "\n",
    "run_logistic(scaler.transform(X_train_original),scaler.transform(X_test_original),y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.9466678206681669\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.910207579508665\n"
     ]
    }
   ],
   "source": [
    "# filter methods - basic\n",
    "scaler = StandardScaler().fit(X_train_basic_filter)\n",
    "\n",
    "run_logistic(scaler.transform(X_train_basic_filter),scaler.transform(X_test_basic_filter),y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.9466678206681669\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.910207579508665\n"
     ]
    }
   ],
   "source": [
    "# filter methods - correlation\n",
    "scaler = StandardScaler().fit(X_train_corr)\n",
    "\n",
    "run_logistic(scaler.transform(X_train_corr),scaler.transform(X_test_corr),y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.9466851306906698\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.9122071986288326\n"
     ]
    }
   ],
   "source": [
    "# filter methods - univariate roc-auc\n",
    "scaler = StandardScaler().fit(X_train[selected_feat.index])\n",
    "\n",
    "run_logistic(scaler.transform(X_train[selected_feat.index]),\n",
    "             scaler.transform(X_test_corr[selected_feat.index]),\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9487450233685303\n",
      "Test set\n",
      "Random Forests roc-auc: 0.9175395162826128\n"
     ]
    }
   ],
   "source": [
    "# embedded methods - Logistic regression coefficients\n",
    "run_randomForests(X_train_coef,\n",
    "                  X_test_coef,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that the selection of features using the feature coefficient led to a better performing logistic regression model (0.795 vs 0.794) compared to univariate feature selection. In addition, the new model has 28 vs 90 features, so it is a win-win :)\n",
    "\n",
    "That is all for this lecture. I hope you enjoyed it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.938999480699325\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.9197295753189868\n"
     ]
    }
   ],
   "source": [
    "# embedded methods - Random Forests importance\n",
    "\n",
    "scaler = StandardScaler().fit(X_train_rf)\n",
    "\n",
    "run_logistic(\n",
    "    scaler.transform(X_train_rf), scaler.transform(X_test_rf), y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Logistic regression we were not so lucky. This makes sense on the other hand, because selection of features by random forest importance, optimises feature selection for tree based methods, as it is able to capture non-linearities that linear regression can not.\n",
    "\n",
    "Therefore, if we are selecting features for a linear model, it is better to use selection procedures targeted to those models, like importance by regression coefficient or Lasso. And if we are selecting features for trees, it is better to use tree derived importance.\n",
    "\n",
    "That is all for this lecture. I hope you enjoyed it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performance using Gradient Boosting Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to build gradient boosted trees\n",
    "# and compare performance in train and test set\n",
    "\n",
    "\n",
    "def run_gradientboosting(X_train, X_test, y_train, y_test):\n",
    "    rf = GradientBoostingClassifier(\n",
    "        n_estimators=200, random_state=39, max_depth=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Train set')\n",
    "    pred = rf.predict_proba(X_train)\n",
    "    print('Random Forests roc-auc: {}'.format(\n",
    "        roc_auc_score(y_train, pred[:, 1])))\n",
    "    print('Test set')\n",
    "    pred = rf.predict_proba(X_test)\n",
    "    print('Random Forests roc-auc: {}'.format(\n",
    "        roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.8580491604639086\n",
      "Test set\n",
      "Random Forests roc-auc: 0.8703104170634165\n"
     ]
    }
   ],
   "source": [
    "# features selected recursively\n",
    "run_gradientboosting(X_train[selected_feat_rfe].fillna(0),\n",
    "                  X_test[selected_feat_rfe].fillna(0),\n",
    "                  y_train, y_test)# features selected altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.8580491604639086\n",
      "Test set\n",
      "Random Forests roc-auc: 0.8703104170634165\n"
     ]
    }
   ],
   "source": [
    "# features selected altogether\n",
    "run_gradientboosting(X_train[selected_feat].fillna(0),\n",
    "                  X_test[selected_feat].fillna(0),\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
